# WSL2 GPU Override for SADNxAI
# Use this with: docker compose -f docker-compose.yml -f docker-compose.wsl.yml up -d
#
# Prerequisites:
# 1. Run Ollama container with GPU separately:
#    docker run -d --gpus all -v ollama:/root/.ollama -p 11434:11434 --name ollama --network sadnxai_default ollama/ollama
# 2. Pull the model:
#    docker exec ollama ollama pull qwen2.5:7b

services:
  # Override ollama to use external container with GPU
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-placeholder
    profiles:
      - disabled
    entrypoint: ["echo", "Using external Ollama container"]

  # Update chat-service to connect to standalone ollama via host network
  chat-service:
    depends_on:
      redis:
        condition: service_started
      masking-service:
        condition: service_started
      validation-service:
        condition: service_started
    extra_hosts:
      - "ollama:host-gateway"
    environment:
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen2.5:7b}
      - REDIS_URL=redis://redis:6379/0
      - MASKING_SERVICE_URL=http://masking-service:8001
      - VALIDATION_SERVICE_URL=http://validation-service:8002
      - STORAGE_PATH=/storage
      - LLM_MOCK_MODE=${LLM_MOCK_MODE:-false}

networks:
  default:
    name: sadnxai_default
    driver: bridge
